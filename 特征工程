机器学习-特征工程

2023年7月24日
8:16

考虑到使用countVectorizer的不便性，尽量找一些关键词（即在某一个类别的文章中，出现的次数很多，但是在其他类别的文章当中很少的词）
方法二：TfidfVectorizer
	TF-IDF的主要思想是：如果某个词汇或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。
	TF-IDF的作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。
	TF:词频，指的是某一个给定的词语在该文件中出现的频率
	IDF ：词语普遍重要性的度量，某一特定词语的idf，可以由总文件数目除以包含该词语的文件的数目，再将得到的商除以10为底的对数得到（就是变为log 商的结果）
	（最终的结果可以理解为某个词汇的重要程度）
"""用TF-IDF的方法进行分词"""
data = [
    "天核心舱舱门开启太空漫步第一翟志刚首位太空教师王亚平第一次出征太空航天员叶光富顺利入驻天开启为期太空生活期间出舱实验遥操作交会对接"]
data_new = []
for sent in data:
    data_new.append(" ".join(list(jieba.cut(sent))))

"""实例化转化器"""
transfer = TfidfVectorizer()
"""调用transfer"""
data_new1 = transfer.fit_transform(data_new)
print("data_new:\n", data_new1.toarray())
"""转成数组的形式导出"""
print("特征名字：\n", transfer.get_feature_names_out())
"""数值越大 表示这个结果越有意义 """

'''结巴分词 加上 Tf-idf的方法使中文特征提取更加便捷 精确'''


二：特征预处理：
通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程
包含内容：数值型数据的无量纲化(消除量纲)：归一化 标准化
特征预处理的API:sklearn.preprocessing
1.归一化：
 
Sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)…)
feature_range=(0,1):放缩的范围
方法：
MinMaxScalar.fit_transform(X)
注：X的格式有要求，必须是numpy array的形式，即二维数组【n_samples,n_features】
返回值：转换后的形状相同的array
读取文件，这里使用Pandas读取
"""归一化处理"""
"""获取数据  读取文本文件"""
data = pd.read_csv("03.txt")
print(data)
'''获取需要的索引值iloc[]'''
data_new = data.iloc[:, :3]
'''获取到前三列 每一行的值'''
print(data_new)

"""实例化转化器类"""
transfer = MinMaxScaler()

"""调用fit_transform方法进行转换"""
data_new1 = transfer.fit_transform(data_new)
print(data_new1)

'''归一化之后 最终输出的结果都在0-1之间 非常合理'''


Python四种读取数据文件的方法：
数据文件的格式一般是：
特征+数据 一一对应（第一行为列名，第一列为编号）

1．	手写读取数据

     f = file(路径名)
     x = []
     y = []
     for i, d in enumerate(f):
enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中
         if i == 0: #跳过第0行的标题
             continue
         d = d.strip()  #去掉换行，回车等
         if not d:  #如果d是空的，没有数据
             continue
         d = list(map(float, d.split(',')))  #d有数据，用，分隔后转为float
         x.append(d[1:-1])  #取第一到倒数第一个之前的数
         y.append(d[-1])  #取倒数第一个数
     print(x)
          print(y)

2.python自带库 使用open函数

    f = open(路径名, 'r')
    print (f)
    d = csv.reader(f)
    for line in d:
        print (line)
f.close()
3.numpy读入

 import numpy as np
p = np.loadtxt(路径名, delimiter=',', skiprows=1)
print (p)

4.Pandas读入
import pandas as pd（pandas安装包要安装）
data = pd.read_csv(路径名)    # TV、Radio、Newspaper、Sales
x = data[['TV', 'Radio', 'Newspaper']]
y = data['Sales']
print (x)
print (y)

但是使用上述方法，需要考虑如果数据有异常值（最大值\最小值），对整个结果的影响非常大，所以只适合传统小数据的统计

2.标准化
 
Mean是均值 ,ŏ是标准差

Sklearn.preprocessin g.StandardScaler()
处理之后，对每列来说，所有的数据都聚集在均值为0附近，标准差为1(正态分布)
StandardScaler.fit_transform(X)
注意，：X的格式有要求，必须是numpy array的形式，即二维数组【n_samples,n_features】
返回值也是转换后的形状相同的array

归一化和标准化的区别：
归一化：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变
标准化：如果出现异常点，由于具有一定的数据量，少量的异常点对于平均值的影响并不大，从而方差改变极小

标准化总结：在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景。

"""数据的标准化"""
data = pd.read_csv("03.txt")
'''获取需要的索引值iloc[]'''
data_new = data.iloc[:, :3]
'''获取到前三列 每一行的值'''

"""实例化转化器类"""
transfer = StandardScaler()
"""调用fit_transform方法进行转换"""
data_new1 = transfer.fit_transform(data_new)
print(data_new1)
print("每一列特征的平均值为：\n",data.mean())
print("每一列特征的标准差是：\n",data.std())


三：特征降维（降低维度）：
维数：嵌套的层数（0维：标量 1维：向量 2维：矩阵 ……）
降维指降低特征的个数
降维的两种方式：
特征选择/主成分分析

什么是特征选择：
数据中包含冗余或相关变量（或称特征、属性、指标等），旨在从原有特征中找出主要特征
Filter（过滤式）：
	1）：方差选择法：低方差特征过滤
	2）：相关系数：特征与特征之间的相关程度·
Embedded(嵌入式)：
	1）：决策树
	2）：正则化
	3)：深度学习

导入模块：sklearn.feature_selection

1.Filter（过滤式）:
1.1:低方差特征过滤：
	删除低方差的一些特征：
		特征方差小：某个特征大多样本的值比较相近（去掉）
特征方差大：某个特征大多样本的值都有差别（保留）
1.2方法：
	  Sklearrn.feature_selection.VarianceThreshold(threshold=0.0)  默认方差threshold为 0
	删除所有低方差的特征
	Variance.fit_transform(X)
	注意：X格式有要求，必须是numpy array的形式，即二维数组【n_samples,n_features】
		返回值：训练集差异低于threshold的特征将被删除，默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征
 
'''获取数据'''
data = pd.read_csv("factor_returns.csv")
'''先选取需要用到的数据'''

data = data.iloc[:, 2:-2]
print(data)
"""注意从1开始倒着数"""
'''实例化转化器'''
transfer = VarianceThreshold(threshold=5)
'''阈值越大过滤掉的更多'''
data_new = transfer.fit_transform(data)
print(data_new)
print("结果大小\n", data_new.shape)
"""用于观察删掉的特征列"""
